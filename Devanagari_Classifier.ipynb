{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEEP_CONVULTIONAL_NET_FOR_DEVANAGARI_CHARACTER_CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORTING_DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOADING_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir_train = 'C://Users//Siddharth//Desktop//AI//devanagari-character-dataset-large//dhcd//train'\n",
    "datadir_test = 'C://Users//Siddharth//Desktop//AI//devanagari-character-dataset-large//dhcd//test'\n",
    "categories = []\n",
    "for i in range(45):\n",
    "    categories.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = []\n",
    "data_test = []\n",
    "def create_train_data():                            #create train and test data \n",
    "    for category in categories:\n",
    "        path_train = os.path.join(datadir_train,category)\n",
    "        class_num = categories.index(category)\n",
    "        for img_train in os.listdir(path_train):\n",
    "            img_array_train = cv2.imread(os.path.join(path_train,img_train), cv2.IMREAD_GRAYSCALE)\n",
    "            data_train.append([img_array_train, class_num])\n",
    "\n",
    "def create_test_data():\n",
    "    for category in categories:\n",
    "        path_test = os.path.join(datadir_test,category)\n",
    "        class_num = categories.index(category)\n",
    "        for img_test in os.listdir(path_test):\n",
    "            img_array_test = cv2.imread(os.path.join(path_test,img_test), cv2.IMREAD_GRAYSCALE)\n",
    "            data_test.append([img_array_test, class_num])            \n",
    "            \n",
    "create_train_data()  \n",
    "create_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76500\n"
     ]
    }
   ],
   "source": [
    "print(len(data_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13500\n"
     ]
    }
   ],
   "source": [
    "print(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random #TO SHUFFLE THE TRAIN AND TEST DATA\n",
    "random.shuffle(data_train)\n",
    "random.shuffle(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []              #initializing lists of train and test data\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features_train, label_train in data_train:   #CREATING A LIST OF TRAIN AND TEST DATA\n",
    "    X_train.append(features_train)\n",
    "    y_train.append(label_train)\n",
    "for features_test, label_test in data_test:\n",
    "    X_test.append(features_test)\n",
    "    y_test.append(label_test)    \n",
    "\n",
    "X_train = np.array(X_train)    #CONVERTING THE LIST TO NUMPY ARRAY\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)    \n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle                           #SAVING AND LOADING THE DATA USING PICKLE MODULE\n",
    "pickle_out = open('X_train.pickle','wb')\n",
    "pickle.dump(X_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open('y_train.pickle','wb')\n",
    "pickle.dump(y_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open('X_test.pickle','wb')\n",
    "pickle.dump(X_test, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open('y_test.pickle','wb')\n",
    "pickle.dump(y_test, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open('X_train.pickle','rb')\n",
    "X_train = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open('y_train.pickle','rb')\n",
    "y_train = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open('X_test.pickle','rb')\n",
    "X_test = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open('y_test.pickle','rb')\n",
    "y_test = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76500, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76500,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(76500,32,32,1).astype('float32')   #RESHAPING TRAIN AND TEST DATA\n",
    "X_test = X_test.reshape(13500,32,32,1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train /= 255                                               #NORMALIZE X_train AND X_train\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 46                                               #DEFINING NUMBER OF CLASSES\n",
    "y_train = keras.utils.to_categorical(y_train,n_classes)\n",
    "y_test = keras.utils.to_categorical(y_test,n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0,        #create a instance of Tensorboard\n",
    "                          write_graph=True, write_images=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATE_SEQUENTIAL_MODEL(CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation = 'relu', input_shape = (32,32,1))) #1st convulutional layer\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))                                             #max pooling layer\n",
    "model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu'))                        #2nd convulutional layer\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))                                             #max polling layer\n",
    "model.add(BatchNormalization())                                                        #batch normalisation layer\n",
    "model.add(Flatten())                                                                   #flattening layer\n",
    "model.add(Dense(128, activation = 'relu'))                                             #1st dense layer\n",
    "model.add(Dropout(0.25))                                                               #dropout layer to avoid overfitting\n",
    "model.add(Dense(n_classes, activation = 'softmax'))                                    #output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               295040    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 46)                5934      \n",
      "=================================================================\n",
      "Total params: 320,046\n",
      "Trainable params: 319,918\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy']) #COMPILING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 76500 samples, validate on 13500 samples\n",
      "Epoch 1/20\n",
      "76500/76500 [==============================] - 51s 661us/step - loss: 0.4461 - acc: 0.8703 - val_loss: 0.1336 - val_acc: 0.9577\n",
      "Epoch 2/20\n",
      "76500/76500 [==============================] - 50s 659us/step - loss: 0.1546 - acc: 0.9525 - val_loss: 0.1421 - val_acc: 0.9599\n",
      "Epoch 3/20\n",
      "76500/76500 [==============================] - 50s 657us/step - loss: 0.1098 - acc: 0.9660 - val_loss: 0.0958 - val_acc: 0.9736\n",
      "Epoch 4/20\n",
      "76500/76500 [==============================] - 50s 658us/step - loss: 0.0870 - acc: 0.9727 - val_loss: 0.1046 - val_acc: 0.97180s - loss: 0.0870 - acc: \n",
      "Epoch 5/20\n",
      "76500/76500 [==============================] - 50s 658us/step - loss: 0.0730 - acc: 0.9768 - val_loss: 0.0974 - val_acc: 0.9733\n",
      "Epoch 6/20\n",
      "76500/76500 [==============================] - 50s 658us/step - loss: 0.0633 - acc: 0.9797 - val_loss: 0.0792 - val_acc: 0.9807\n",
      "Epoch 7/20\n",
      "76500/76500 [==============================] - 50s 656us/step - loss: 0.0567 - acc: 0.9820 - val_loss: 0.0781 - val_acc: 0.9805\n",
      "Epoch 8/20\n",
      "76500/76500 [==============================] - 50s 654us/step - loss: 0.0506 - acc: 0.9836 - val_loss: 0.0920 - val_acc: 0.9783\n",
      "Epoch 9/20\n",
      "76500/76500 [==============================] - 50s 657us/step - loss: 0.0440 - acc: 0.9862 - val_loss: 0.0816 - val_acc: 0.9813\n",
      "Epoch 10/20\n",
      "76500/76500 [==============================] - 50s 657us/step - loss: 0.0436 - acc: 0.9861 - val_loss: 0.0760 - val_acc: 0.9827\n",
      "Epoch 11/20\n",
      "76500/76500 [==============================] - 50s 655us/step - loss: 0.0418 - acc: 0.9872 - val_loss: 0.0793 - val_acc: 0.9829\n",
      "Epoch 12/20\n",
      "76500/76500 [==============================] - 50s 656us/step - loss: 0.0372 - acc: 0.9882 - val_loss: 0.0994 - val_acc: 0.9791\n",
      "Epoch 13/20\n",
      "76500/76500 [==============================] - 50s 655us/step - loss: 0.0323 - acc: 0.9898 - val_loss: 0.0855 - val_acc: 0.9819\n",
      "Epoch 14/20\n",
      "76500/76500 [==============================] - 50s 655us/step - loss: 0.0339 - acc: 0.9896 - val_loss: 0.0843 - val_acc: 0.9836\n",
      "Epoch 15/20\n",
      "76500/76500 [==============================] - 50s 656us/step - loss: 0.0347 - acc: 0.9899 - val_loss: 0.0935 - val_acc: 0.9816\n",
      "Epoch 16/20\n",
      "76500/76500 [==============================] - 50s 658us/step - loss: 0.0302 - acc: 0.9903 - val_loss: 0.0950 - val_acc: 0.9818\n",
      "Epoch 17/20\n",
      "76500/76500 [==============================] - 50s 657us/step - loss: 0.0279 - acc: 0.9910 - val_loss: 0.0796 - val_acc: 0.9830\n",
      "Epoch 18/20\n",
      "76500/76500 [==============================] - 50s 659us/step - loss: 0.0263 - acc: 0.9921 - val_loss: 0.0828 - val_acc: 0.9835\n",
      "Epoch 19/20\n",
      "76500/76500 [==============================] - 51s 661us/step - loss: 0.0246 - acc: 0.9924 - val_loss: 0.0792 - val_acc: 0.9844\n",
      "Epoch 20/20\n",
      "76500/76500 [==============================] - 50s 659us/step - loss: 0.0242 - acc: 0.9922 - val_loss: 0.0785 - val_acc: 0.9839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cb386d8d68>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size = 32, epochs = 20, verbose = 1, validation_data = [X_test, y_test], callbacks=[tensorboard])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
